{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "available but incensistence\n",
    "or\n",
    "not available but consistent\n",
    "nodes have data inconsistent caused by network fall \n",
    "partition tolerance \n",
    "no distributributed system is immune to netwark failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAP theorem \n",
    "can only gaurantee 2o3\n",
    "consistency\n",
    "avail\n",
    "partition tolerance\n",
    "\n",
    "2 ways \n",
    "- cancel the operation\n",
    "- proceed with operation\n",
    "\n",
    "\n",
    "transactions: a complete request (chunk of code) get to be spill or kill, \n",
    "\n",
    "Major CP or AP\n",
    "for redundency purpose, a same data can locate at different nodes, also, data can sit at headquater / local centers\n",
    "CP: need a monitor, assign a leader that function correctly now, and rest followers,  \n",
    "    downside - wait to deal with every partition\n",
    "AP: eventually failed nodes need to catch up, but old data can be available to users\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the catelyst is the engine driving spark SQL\n",
    "\n",
    "the catalyst engine uses tree as the data type for an expression\n",
    "define rules to run operation, rules to execute, tree is not changing then work is done, \n",
    "fix point when input(tree) and output(tree) are the same, x=y\n",
    "\n",
    "catalyst create the abstract syntax tree(AST) spark feature to compilers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The catalyst serves as Spark SQL's driving engine, utilizing a tree structure for expression data types. It establishes rules for operation execution, striving for a fixed point where input and output trees are identical (x=y). Additionally, the catalyst plays a crucial role in constructing the Abstract Syntax Tree (AST), a key feature that help boost compiler infrastructures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.query(\"column_A == 'hahha'\")[['column_B', 'column_C']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first merge, sanity check\n",
    "df_merged = pd.merge(df1, df2,\n",
    "        on= 'column_key',\n",
    "        how = 'outer',\n",
    "        validate = 'many_to_one',\n",
    "        indicator = 'matched')\n",
    "df_merged['matched'].value_counts()\n",
    "# second merge, do it more memory efficient\n",
    "df_merged = pd.merge(df1, df2,\n",
    "        on= 'column_key',\n",
    "        how = 'inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User defined function in pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
