{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Journaling \n",
    "#### Jiaxing (Joy) Qiu\n",
    "#### JQ2UW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What were some things you learned in the module?\n",
    "- What do you think were the most important concepts?\n",
    "- What was challenging for you? How can you learn it better?\n",
    "- Which parts did you enjoy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I learned a broadview concept of cloud, which is providor companies allocate hardware or software servers to users who pay for the service. Combining the uage of more servers and balancing the load from user requests is efficient and inexpensive way to scale computing resources (term: horizontal scaling). Big data processig include majors steps of data collection, storage, processing and front-end reporting/visualing. \n",
    "I learned the MapReduce programming framework. Big dataset is splitted and sent to multiple workers of differnt machines, higher-order calculations/functions (in forms of distributed algorithm codes) are sent to each split and \"mapped\" to each entry; the final result is \"reduced\" by directly aggregating entries from all mapping workers, or indirectly reorganizing the results from mapping workers.\n",
    "\n",
    "\n",
    "\n",
    "The horizontal scaling, MapReduce programming framework, and that keypairs are critical structure for MapReduce to work, were the most important concepts to me.\n",
    "Understanding clsuter and cloud structure is challenging. Reading materials and class contents would help. \n",
    "Pondering about the workflows of cloud and MapReduce framework, drawing a dynamic data workflow in mind based on what I could understand so far.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Notes from \"learning PySpark -- Chapter 1. Spark Jobs and APIs\"\n",
    "\n",
    "1. Any spark application split a single master node to multiple worker nodes, and split jobs on the master node to tasks on each worker node.\n",
    "2. Spark can optimaze the scheduling (determine # of taks and workers required) and how to execute these tasks.\n",
    "3. JVM: Java virtual machines. they all jobs to perform very quickly.\n",
    "4. python data are all stored in JVM objects. \n",
    "5. RDD: a data structure, it canprovides data lineage (ancestry tree) for data recovery from data loss, slower in python than java or scala\n",
    "6. DataFrame: performance parity across all languages.\n",
    "\n",
    "Notes from \"learning PySpark -- Chapter 2. Resilient Distributed Datasets\"\n",
    "\n",
    "1. RDD -- backbone of spark. Disttributed colleciton of immutable JVM objects\n",
    "2. Whenever you can, you should use built-in Spark functions.\n",
    "3. running spark locally is same as current running in python by changes are syntatic.\n",
    "4. running spark in cluster: trickier to track down fault from nodes.\n",
    "5. Definitions:\n",
    "    - reliability: system's ability to work correctly (performing the correct function at the desired level of performance) even in the face of adversity (tolerate or prevent hardware or software faults). \n",
    "    - scalability: reasonable way to deal with system's growth such as increased load (data volume, traffic volume, or complexity).\n",
    "    - throughput: number of records been processed per second.\n",
    "    - latency: the duration that a request id waiting to be handled (during which it is awaiting service). (the response time is from the client point of view, the whole actual time to process the request, including network delay and queueing delays).\n",
    "    \n",
    "    \n",
    "Performance\n",
    "\n",
    "operational:\n",
    "- latency: from request to result (time until we result)\n",
    "- uptime: available time from service, stretch of time uninterrupted\n",
    "- throughput\n",
    "\n",
    "predictive:\n",
    "- ML metrics\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module, I learned concepts about distributed data system, including the what reliability and scalability of a system are, ways to enhance these properties, as well as the definitions of metrics that measure operational performance of a system, such as latency, uptime and throughput. Meanwhile, I have a clearer idea of who Spark paralleize data processing either locally or on a cloud/cluster. The design of a cluster include a driver machine/node, and several worker machines/nodes, as well as a cluster manager communicating between the driver and the workers. Each worker node can have one or more cores to work on tasks, and task data are temporarily stored on worker machines' caches.  The most important concept underneath Spark to me is the RDD structure, it makes MapReduce framework possible in practice. \n",
    "\n",
    "The challenging part is getting familiar with PySpark package and RDD object in python, I can learn better by solving problems in assignments, though playing with string and regular expression is not fun >o<. I am happy to know that RDD (JVM object) is the underlying structrue of pandas DataFrame or numpy array, which maybe a reason those packages run fast and efficient! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module, I learned 1) useful operations on spark Dataset and DataFrame. It can do basically all functions on a DataFrame (with schema) like pandas but allows data partitions being distributed and processed parallely, which is super efficient when we want to optimize operations on massive data. It also allows Spark SQL language queries that facilicates the communications cross human teams and softwares. We want to avoid for-loop because it will damage parallel processing; Partitioning data can be done in an informative way, that is, data can be split by given categorical fields. 2) Besides types of database, Parquet is an efficient columnar format of data, with metadata provided to store in may systems, reading and writting Parquet can be much faster in spark, and efficient for ML projects where we don't need to access the entire row all time; \n",
    "\n",
    "The challenging part is getting familiar with pyspark package, which should improve over time when I get to use it more often in course work and personal projects. The high moment of this week -- Spark could be my new favor and go-to toolbox for data processing and engineering, divorcing pandas now (not really)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module, I learned the catelyst engine that drives spark SQL, it uses tree data type/expression and employs query optimization techniques like join reordering and cost-based optimization to enhance query performance, resulting in faster query execution. For scala or R, catalyst create the abstract syntax tree(AST) spark feature to programming languages that use compilers. \n",
    "\n",
    "I enjoyed learning CAP theorem for cloud/distributed systems design. We can only guarantee 2 out of 3 of the following properties of a system: consistency (user can only access synchronized accurate and updated data), availability (data are availble all time but might have out-of-date errors) and partition tolerance (the system continues to function while partition of the nodes mal-function.). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module, I learned methods to do unsupervised learning in spark MLlib, it's a great chance to review dimensionality reduction strategies that shrink data space across samples/observartions (k-means clustering) or across original column/variable space (PCA). I am glad to learn evaluation matrices such as Silhouette Score, and algorithm that's new to me Latent Dirichlet allocation in topic modeling, and efficient algorithms to do K-means parallelly. \n",
    "\n",
    "Not too much challenging to me for now (which means I am not yet deep in this rabbit hole), but unsupervised methodologies are very intriguing to me, especially methods to generate latent space with reduced dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module, I learned about recommender systems and more pyspark MLlib package utilities to feature data processing pipeline. The package incorporates feature extraction, transformation, and selection utilities. I enjoyed learning how to construct machine learning pipelines (actually first time really using it), conceptually it can help enhance reproducibility, and make the model development and validation in a cleaner and faster way YAY. I also had a hands-on experience of building a recommender system through assignment.\n",
    "\n",
    "The challenge I had was majorly understanding what a recommendar system is and how it works. I read package documentation while constructing a recommender system which helped me learn it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module, I learned key concepts related to model selection, hyperparameter tuning, and cross-validation in the context of machine learning with Apache Spark. It's important to distinguish an estimator, whcih represents the algorithm or pipeline to be tuned, and an\n",
    "evaluator, which is a metric used to measure how well a fitted model performs on held-out test data.\n",
    "\n",
    "Not much challenging for this topic, it's great to have a pyspark option when building up ML pipeline and developing certain ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that module, I learned a range of Amazon Web Services (AWS) tools designed for large-scale data processing, storage, and retrieval. Amazon EC2 enables cloud computing, and Amazon S3 provides scalable storage. Then, I read about AWS Glue, a tool for automating ETL, data preprocessing, and data cataloging. Amazon Athena supports big data analytics and SQL queries at scale. AWS placed a strong emphasis on security, and I gained knowledge about policies and leader principles.\n",
    "\n",
    "By the end of that module, I was able to utilize EC2 for computing, configured and launched Amazon EC2 instances, understood various options for big data storage on AWS, recognized the advantages of S3, effectively worked with S3 buckets, created IAM roles and policies. I am glad to be equipped with tools to execute predictive modeling projects using a large dataset on a cloud-based platform. Challenges probably lies in figuring out how to use individual AWS tools. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10\n",
    "\n",
    "batch processing is fundmantally easier: a finite storage,\n",
    "\n",
    "streaming is inifite data need to save and process right away, and need to decide when to store process and report. \n",
    "\n",
    "microbatch is spark streaming solution for processing streaming data, there are RDDs at each time unit of processing steps, \n",
    "\n",
    "\n",
    "many sources pipe data into a producer, producer produces stream of data, and filter can do certain things such like looking for hashtags keywords (like remove danger words), consumer can culculate results like keyworks of interest(like finding components for alert)\n",
    "\n",
    "\n",
    "running mean: find sum and number \n",
    "running variance: sum, sum of squares, n, sum of x*x_mean\n",
    "\n",
    "running histogram: count within each bin, accumulate over time\n",
    "\n",
    "\n",
    "the lag between event time and proc time could be the time it take to upload data from player phone to the server (the connection might break),\n",
    "or there is a intermediate play in the processing that wait for additional information\n",
    "\n",
    "\n",
    "trade-off latency versus accuracy: i.e. real time high score for any player, but not all players get their data arrived at the server instantly, how long should we wait.\n",
    "\n",
    "\n",
    "how complete is the data: concept of the watermark -- find a time window when complete data are collected. but it doesn't happen all the time, the lateness, option1: update stocks(model) with current data and do a 2nd update later; option 2: update stocks(model) and skip the rest; option 3: wait for a few more hours, and skip the rest. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS5110",
   "language": "python",
   "name": "ds5110"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
