{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commercial Data Analysis\n",
    "\n",
    "### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "### Last Updated: February 9, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS  \n",
    "In this assignment, you will work with a dataset containing information about businesses.  Each record is a business location.  Follow the steps below, writing and running the code in blocks, and displaying the solutions.  \n",
    "\n",
    "Each question part is worth 1 POINT, for a total of 15 POINTS.\n",
    "\n",
    "Hint: reaching deeper fields in json hierarchy can be done like this:  \n",
    "\n",
    "`df.select('address.street_number')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ccc647018b91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"comm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"comm\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/gpfs/gpfs0/project/SDS/instructional/ds5559/large_datasets'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that read.json can read a zipped JSON directly\n",
    "df = spark.read.json('part-00000-a159c41a-bc58-4476-9b78-c437667f9c2b-c000.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----+----------------+----+-------+--------------------+--------------------+\n",
      "|             address|business_tags|hours|              id|menu|reviews|                urls|             webpage|\n",
      "+--------------------+-------------+-----+----------------+----+-------+--------------------+--------------------+\n",
      "|{Woodburn, {45.15...|         null| null|000023995a540868|null|     []|{woodburn.k12.or....|{Educational Tech...|\n",
      "+--------------------+-------------+-----+----------------+----+-------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. (1 PT) Read in the dataset and show the number of records**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of records: 154679\n"
     ]
    }
   ],
   "source": [
    "record_count = df.count()\n",
    "print(\"Count of records:\", record_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. (1 PT) Print the schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- coordinates: struct (nullable = true)\n",
      " |    |    |-- lat: double (nullable = true)\n",
      " |    |    |-- lon: double (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- county: string (nullable = true)\n",
      " |    |-- full_address: string (nullable = true)\n",
      " |    |-- highway_number: string (nullable = true)\n",
      " |    |-- is_headquarters: boolean (nullable = true)\n",
      " |    |-- is_parsed: boolean (nullable = true)\n",
      " |    |-- post_direction: string (nullable = true)\n",
      " |    |-- pre_direction: string (nullable = true)\n",
      " |    |-- secondary_number: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |    |-- street: string (nullable = true)\n",
      " |    |-- street_address: string (nullable = true)\n",
      " |    |-- street_number: string (nullable = true)\n",
      " |    |-- street_type: string (nullable = true)\n",
      " |    |-- type_of_address: string (nullable = true)\n",
      " |    |-- zip: string (nullable = true)\n",
      " |    |-- zip_suffix: string (nullable = true)\n",
      " |-- business_tags: struct (nullable = true)\n",
      " |    |-- no: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- tags: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- value: string (nullable = true)\n",
      " |    |-- yes: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- hours: struct (nullable = true)\n",
      " |    |-- any_day_is_24: boolean (nullable = true)\n",
      " |    |-- friday_close: string (nullable = true)\n",
      " |    |-- friday_lb: long (nullable = true)\n",
      " |    |-- friday_open: string (nullable = true)\n",
      " |    |-- friday_total_seconds: long (nullable = true)\n",
      " |    |-- hours: struct (nullable = true)\n",
      " |    |    |-- Friday: string (nullable = true)\n",
      " |    |    |-- Monday: string (nullable = true)\n",
      " |    |    |-- Saturday: string (nullable = true)\n",
      " |    |    |-- Sunday: string (nullable = true)\n",
      " |    |    |-- Thursday: string (nullable = true)\n",
      " |    |    |-- Tuesday: string (nullable = true)\n",
      " |    |    |-- Wednesday: string (nullable = true)\n",
      " |    |-- monday_close: string (nullable = true)\n",
      " |    |-- monday_lb: long (nullable = true)\n",
      " |    |-- monday_open: string (nullable = true)\n",
      " |    |-- monday_total_seconds: long (nullable = true)\n",
      " |    |-- saturday_close: string (nullable = true)\n",
      " |    |-- saturday_lb: long (nullable = true)\n",
      " |    |-- saturday_open: string (nullable = true)\n",
      " |    |-- saturday_total_seconds: long (nullable = true)\n",
      " |    |-- sunday_close: string (nullable = true)\n",
      " |    |-- sunday_lb: long (nullable = true)\n",
      " |    |-- sunday_open: string (nullable = true)\n",
      " |    |-- sunday_total_seconds: long (nullable = true)\n",
      " |    |-- thursday_close: string (nullable = true)\n",
      " |    |-- thursday_lb: long (nullable = true)\n",
      " |    |-- thursday_open: string (nullable = true)\n",
      " |    |-- thursday_total_seconds: long (nullable = true)\n",
      " |    |-- tuesday_close: string (nullable = true)\n",
      " |    |-- tuesday_lb: long (nullable = true)\n",
      " |    |-- tuesday_open: string (nullable = true)\n",
      " |    |-- tuesday_total_seconds: long (nullable = true)\n",
      " |    |-- wednesday_close: string (nullable = true)\n",
      " |    |-- wednesday_lb: long (nullable = true)\n",
      " |    |-- wednesday_open: string (nullable = true)\n",
      " |    |-- wednesday_total_seconds: long (nullable = true)\n",
      " |    |-- week_total_hours_pretty: string (nullable = true)\n",
      " |    |-- week_total_minutes_pretty: string (nullable = true)\n",
      " |    |-- week_total_seconds: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- menu: struct (nullable = true)\n",
      " |    |-- price_range: string (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |-- reviews: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- content: string (nullable = true)\n",
      " |    |    |-- date: string (nullable = true)\n",
      " |    |    |-- dislikes: long (nullable = true)\n",
      " |    |    |-- gender: string (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- language: string (nullable = true)\n",
      " |    |    |-- likes: long (nullable = true)\n",
      " |    |    |-- source: string (nullable = true)\n",
      " |    |    |-- stars: long (nullable = true)\n",
      " |    |    |-- tags: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- user: string (nullable = true)\n",
      " |    |    |-- user_id: string (nullable = true)\n",
      " |-- urls: struct (nullable = true)\n",
      " |    |-- domain: string (nullable = true)\n",
      " |    |-- domains: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- urls: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- webpage: struct (nullable = true)\n",
      " |    |-- content: string (nullable = true)\n",
      " |    |-- count: long (nullable = true)\n",
      " |    |-- elapsed: double (nullable = true)\n",
      " |    |-- success: boolean (nullable = true)\n",
      " |    |-- timestamp: string (nullable = true)\n",
      " |    |-- title: string (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- urlhash: string (nullable = true)\n",
      " |    |-- validurl: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. (1 PT) Show the first 5 records**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------------+----+--------------------+--------------------+--------------------+\n",
      "|             address|       business_tags|               hours|              id|menu|             reviews|                urls|             webpage|\n",
      "+--------------------+--------------------+--------------------+----------------+----+--------------------+--------------------+--------------------+\n",
      "|{Woodburn, {45.15...|                null|                null|000023995a540868|null|                  []|{woodburn.k12.or....|{Educational Tech...|\n",
      "|{Hialeah, {25.884...|{[], [{has_atm, Y...|{null, 1900, null...|0000821a1394916e|null|                null|{null, [yelp.com]...|                null|\n",
      "|{Rochester, {43.1...|{[], [{accepts_cr...|{null, 1700, null...|000136e65d50c3b7|null|[{New (to me) qui...|{usps.com, [yelp....|{Welcome | USPS G...|\n",
      "|{West Palm Beach,...|                null|                null|00014329a70b9869|null|                null|                null|                null|\n",
      "|{Eufaula, {35.283...|                null|{null, 1700, null...|00031c0a83f00657|null|                null|{drsodomcoburnand...|{DRS. COBURN, RIC...|\n",
      "+--------------------+--------------------+--------------------+----------------+----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. (1 PT) Location**  \n",
    "\n",
    "Count the number of records where the city is Houston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of records where the city is Houston: 1668\n"
     ]
    }
   ],
   "source": [
    "houston_count = df.filter(col(\"address.city\") == \"Houston\").count()\n",
    "print(\"Count of records where the city is Houston:\", houston_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. (1 PT) Hours**  \n",
    "\n",
    "Count the number of records where closing time on Friday is 7pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of records where closing time on Friday is 7pm: 3305\n"
     ]
    }
   ],
   "source": [
    "friday_closing_count = df.filter(col(\"hours.friday_close\") == \"1900\").count()\n",
    "print(\"Count of records where closing time on Friday is 7pm:\", friday_closing_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. (1 PT) Location and Hours**  \n",
    "\n",
    "Count the number of records where city is Houston and closing time on Friday is 7pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of records where city is Houston and closing time on Friday is 7pm: 42\n"
     ]
    }
   ],
   "source": [
    "houston_friday_closing_count = df.filter((col(\"address.city\") == \"Houston\") & (col(\"hours.friday_close\") == \"1900\")).count()\n",
    "print(\"Count of records where city is Houston and closing time on Friday is 7pm:\", houston_friday_closing_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. (1 PT) Price Range**  \n",
    "\n",
    "Price range is quoted in number of dollar signs.  Count the number of records with price range greater than or equal to three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of records with price range greater than or equal to three: 115\n"
     ]
    }
   ],
   "source": [
    "price_range_count = df.filter((col(\"menu.price_range\")) >= 3).count()\n",
    "print(\"Count of records with price range greater than or equal to three:\", price_range_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. (1 PT) COMPANY HEADQUARTERS**  \n",
    "\n",
    "For the `address.is_headquarters` field:  \n",
    "how many locations are HQ / are NOT HQ / are null?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of locations that are HQ: 318\n",
      "Count of locations that are NOT HQ: 66736\n",
      "Count of locations where is_headquarters field is null: 87625\n"
     ]
    }
   ],
   "source": [
    "# Count the number of locations that are HQ\n",
    "hq_count = df.filter(col(\"address.is_headquarters\") == True).count()\n",
    "\n",
    "# Count the number of locations that are NOT HQ\n",
    "not_hq_count = df.filter(col(\"address.is_headquarters\") == False).count()\n",
    "\n",
    "# Count the number of locations where is_headquarters field is null\n",
    "null_hq_count = df.filter(col(\"address.is_headquarters\").isNull()).count()\n",
    "\n",
    "print(\"Count of locations that are HQ:\", hq_count)\n",
    "print(\"Count of locations that are NOT HQ:\", not_hq_count)\n",
    "print(\"Count of locations where is_headquarters field is null:\", null_hq_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. (1 PT) Webpage URLs**  \n",
    "\n",
    "Register the dataframe as a temp table.  \n",
    "Next, use Spark SQL to select only the webpage title column, filtering on rows where the webpage url (accessed under `webpage.url`) is *Target.com*. \n",
    "\n",
    "Show only one resulting row and don't truncate the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"business_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|title                          |\n",
      "+-------------------------------+\n",
      "|Target : Expect More. Pay Less.|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "query = \"SELECT webpage.title FROM business_data WHERE webpage.url = 'Target.com' LIMIT 1\"\n",
    "result = spark.sql(query)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. (1 PT) Analysis on Ratings**  \n",
    "\n",
    "The reviews contains information such as the number of stars for each review (the *rating*).  \n",
    "The ratings are stored in an array (`reviews.stars`) for each business location (you should check for yourself). Return the top five most common rating arrays.  For example, an array might look like: \n",
    "[5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|rating|\n",
      "+------+\n",
      "|     4|\n",
      "|     4|\n",
      "|     5|\n",
      "|  null|\n",
      "|  null|\n",
      "|  null|\n",
      "|  null|\n",
      "|  null|\n",
      "|  null|\n",
      "|  null|\n",
      "|  null|\n",
      "|  null|\n",
      "|  null|\n",
      "|  null|\n",
      "|  null|\n",
      "|     1|\n",
      "|     5|\n",
      "|     2|\n",
      "|     4|\n",
      "|     5|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploded_ratings = df.select(explode(\"reviews.stars\").alias(\"rating\"))\n",
    "exploded_ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, array_sort, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|stars |count|\n",
      "+------+-----+\n",
      "|null  |74679|\n",
      "|[]    |42419|\n",
      "|[5]   |4258 |\n",
      "|[null]|3067 |\n",
      "|[5, 5]|1610 |\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rating_arrays = df.groupBy(\"reviews.stars\").count()\n",
    "rating_arrays = rating_arrays.sort(desc(\"count\"))\n",
    "top_five_ratings = rating_arrays.limit(5)\n",
    "top_five_ratings.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. More work with Ratings**  \n",
    "\n",
    "For this question, you will filter out null ratings and then compute the average rating for each business location (using the field: `id`).\n",
    "\n",
    "\n",
    "a) (1 PT) Create a new dataframe retaining two fields: `id`, `reviews.stars`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|              id|               stars|\n",
      "+----------------+--------------------+\n",
      "|000023995a540868|                  []|\n",
      "|000136e65d50c3b7|              [4, 4]|\n",
      "|0003b7589a4e12a0|                 [5]|\n",
      "|00045f958e4bb02a|[null, null, null...|\n",
      "|00059519f0dba1b4|[null, null, null...|\n",
      "|0006d5aa170bae22|                  []|\n",
      "|0008bc70f8ba62bf|              [null]|\n",
      "|000a1df4c8e0ecd2|[null, null, 4, 5...|\n",
      "|000bf1e934ac9cb6|                  []|\n",
      "|000c4037ef6d4b3b|                  []|\n",
      "|000c7b7a30623083|                 [5]|\n",
      "|000c9ffc8b89af03|[5, 2, 5, 3, 3, 1...|\n",
      "|000ca67c3cf252e5|                  []|\n",
      "|000de20baa847ecc|  [1, 1, 1, 1, 5, 1]|\n",
      "|000e439e7667839d|                  []|\n",
      "|001064359d9f162f|     [5, 5, 5, 5, 5]|\n",
      "|0010c9f495d87dd7|[5, 1, 1, 5, 3, 5...|\n",
      "|0012eac5aaf0bd45|                  []|\n",
      "|0013cd52c783f818|                  []|\n",
      "|0017774db5e6400a|[null, 5, 5, 5, 5...|\n",
      "+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df = df.filter(col(\"reviews.stars\").isNotNull()).select(\"id\", \"reviews.stars\")\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) (1 PT) Create a row for each rating  \n",
    "hint: use the `withColumn()` and `explode()` functions  \n",
    "you will need to import the `explode()` function by issuing:\n",
    "\n",
    "`from pyspark.sql.functions import explode`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+------+\n",
      "|             address|       business_tags|               hours|              id|                menu|             reviews|                urls|             webpage|rating|\n",
      "+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+------+\n",
      "|{Rochester, {43.1...|{[], [{accepts_cr...|{null, 1700, null...|000136e65d50c3b7|                null|[{New (to me) qui...|{usps.com, [yelp....|{Welcome | USPS G...|     4|\n",
      "|{Rochester, {43.1...|{[], [{accepts_cr...|{null, 1700, null...|000136e65d50c3b7|                null|[{New (to me) qui...|{usps.com, [yelp....|{Welcome | USPS G...|     4|\n",
      "|{Birmingham, {33....|                null|                null|0003b7589a4e12a0|                null|[{Dr Cox and his ...|{null, [google.co...|                null|     5|\n",
      "|{Houston, {29.722...|{[], [{takes_rese...|{null, 2400, null...|00045f958e4bb02a|{2, https://www.y...|[{Dine in I resca...|{pizzahut.com, [f...|                null|  null|\n",
      "|{Houston, {29.722...|{[], [{takes_rese...|{null, 2400, null...|00045f958e4bb02a|{2, https://www.y...|[{Dine in I resca...|{pizzahut.com, [f...|                null|  null|\n",
      "|{Houston, {29.722...|{[], [{takes_rese...|{null, 2400, null...|00045f958e4bb02a|{2, https://www.y...|[{Dine in I resca...|{pizzahut.com, [f...|                null|  null|\n",
      "|{Houston, {29.722...|{[], [{takes_rese...|{null, 2400, null...|00045f958e4bb02a|{2, https://www.y...|[{Dine in I resca...|{pizzahut.com, [f...|                null|  null|\n",
      "|{Helen, {34.68952...|{[by_appointment_...|{null, 1800, null...|00059519f0dba1b4|                null|[{Dishware, decor...|{facebook.com, [f...|                null|  null|\n",
      "|{Helen, {34.68952...|{[by_appointment_...|{null, 1800, null...|00059519f0dba1b4|                null|[{Dishware, decor...|{facebook.com, [f...|                null|  null|\n",
      "|{Helen, {34.68952...|{[by_appointment_...|{null, 1800, null...|00059519f0dba1b4|                null|[{Dishware, decor...|{facebook.com, [f...|                null|  null|\n",
      "|{Helen, {34.68952...|{[by_appointment_...|{null, 1800, null...|00059519f0dba1b4|                null|[{Dishware, decor...|{facebook.com, [f...|                null|  null|\n",
      "|{Helen, {34.68952...|{[by_appointment_...|{null, 1800, null...|00059519f0dba1b4|                null|[{Dishware, decor...|{facebook.com, [f...|                null|  null|\n",
      "|{Helen, {34.68952...|{[by_appointment_...|{null, 1800, null...|00059519f0dba1b4|                null|[{Dishware, decor...|{facebook.com, [f...|                null|  null|\n",
      "|{Helen, {34.68952...|{[by_appointment_...|{null, 1800, null...|00059519f0dba1b4|                null|[{Dishware, decor...|{facebook.com, [f...|                null|  null|\n",
      "|{Helen, {34.68952...|{[by_appointment_...|{null, 1800, null...|00059519f0dba1b4|                null|[{Dishware, decor...|{facebook.com, [f...|                null|  null|\n",
      "|{Helen, {34.68952...|{[by_appointment_...|{null, 1800, null...|00059519f0dba1b4|                null|[{Dishware, decor...|{facebook.com, [f...|                null|     1|\n",
      "|{Helen, {34.68952...|{[by_appointment_...|{null, 1800, null...|00059519f0dba1b4|                null|[{Dishware, decor...|{facebook.com, [f...|                null|     5|\n",
      "|{Helen, {34.68952...|{[by_appointment_...|{null, 1800, null...|00059519f0dba1b4|                null|[{Dishware, decor...|{facebook.com, [f...|                null|     2|\n",
      "|{Helen, {34.68952...|{[by_appointment_...|{null, 1800, null...|00059519f0dba1b4|                null|[{Dishware, decor...|{facebook.com, [f...|                null|     4|\n",
      "|{Helen, {34.68952...|{[by_appointment_...|{null, 1800, null...|00059519f0dba1b4|                null|[{Dishware, decor...|{facebook.com, [f...|                null|     5|\n",
      "+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expanded_df = df.withColumn(\"rating\", explode(\"reviews.stars\"))\n",
    "expanded_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) (1 PT) Return a count of the number of ratings in this dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of ratings: 600082\n"
     ]
    }
   ],
   "source": [
    "rating_count = expanded_df.count()\n",
    "print(\"Count of ratings:\", rating_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) (1 PT) Drop rows where the rating is null, and return a count of the number of non-null ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of non-null ratings: 538241\n"
     ]
    }
   ],
   "source": [
    "non_null_rating_count = expanded_df.na.drop(subset=[\"rating\"]).count()\n",
    "print(\"Count of non-null ratings:\", non_null_rating_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) (1 PT) Compute the average rating, grouped by `id`. After the average is computed, sort by `id` in ascending order and show the top 10 records.  \n",
    " \n",
    "hint:   \n",
    "this can all be done in one line using the `agg()` function  \n",
    "this `id` should be at the top: 000136e65d50c3b7|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n",
      "|              id|    average_rating|\n",
      "+----------------+------------------+\n",
      "|000136e65d50c3b7|               4.0|\n",
      "|0003b7589a4e12a0|               5.0|\n",
      "|00045f958e4bb02a|              null|\n",
      "|00059519f0dba1b4|3.3333333333333335|\n",
      "|0008bc70f8ba62bf|              null|\n",
      "|000a1df4c8e0ecd2|               4.6|\n",
      "|000c7b7a30623083|               5.0|\n",
      "|000c9ffc8b89af03|               3.0|\n",
      "|000de20baa847ecc|1.6666666666666667|\n",
      "|001064359d9f162f|               5.0|\n",
      "+----------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = expanded_df.groupBy(\"id\").agg(avg(\"rating\").alias(\"average_rating\")).orderBy(\"id\")\n",
    "result.show(10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
