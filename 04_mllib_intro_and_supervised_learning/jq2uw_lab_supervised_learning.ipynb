{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Lab: Supervised Learning\n",
    "### Last Updated: August 20, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jq2uw \n",
    "## Jiaxing Qiu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "This project has two parts:\n",
    "- Part I: Classification - build and apply a logistic regression model on the Wisconsin Breast Cancer dataset.\n",
    "- Part II: Regression - build and apply a linear regression model on the California Housing dataset.\n",
    "\n",
    "**Total Possible Points: 10**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part I: Classification (5 POINTS)\n",
    "\n",
    "Here are the specifications and grading breakdown:\n",
    "\n",
    "- the target variable is `diagnosis`\n",
    "- use `f1`, `f2` as predictors (1 PT)\n",
    "- split data into 60% training set, 40% test set \n",
    "- standardize the predictors (1 PT)\n",
    "- use seed=314 whenever a seed is needed\n",
    "- fit a Logistic Regression model with an intercept (1 PT)\n",
    "- compute and show the area under the ROC curve for the test set (2 PTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter code and solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_FILEPATH = 'wisc_breast_cancer_w_fields.csv'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Wisc BRCA\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame:\n",
      "+----+---+\n",
      "|  id|gid|\n",
      "+----+---+\n",
      "|8670|131|\n",
      "|8913|287|\n",
      "+----+---+\n",
      "only showing top 2 rows\n",
      "\n",
      "Test DataFrame:\n",
      "+--------+---+\n",
      "|      id|gid|\n",
      "+--------+---+\n",
      "|84348301|  3|\n",
      "|84799002| 15|\n",
      "+--------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = spark.read.csv(DATA_FILEPATH, header=True, inferSchema=True)\n",
    "#data.show(2)\n",
    "\n",
    "# ---- global engineering X ----\n",
    "# assemble data \n",
    "assembler = VectorAssembler(inputCols=[\"f\" + str(i) for i in range(1, 3)],\n",
    "                            outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "#data.show(2, truncate=False)\n",
    "\n",
    "# ---- train/test split ----\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.sql import functions as F\n",
    "train_ratio = 0.6\n",
    "\n",
    "# use row index as id per observation\n",
    "data = data.withColumn(\"row_id\", F.monotonically_increasing_id())\n",
    "#data.select(\"id\",\"row_id\").show(20)\n",
    "\n",
    "# get unique id per patient\n",
    "grouped_data = data.groupBy(\"id\").agg(F.max(\"row_id\").alias(\"gid\")).sort(\"gid\")\n",
    "#grouped_data.show(10)\n",
    "\n",
    "# Use randomSplit to split the data\n",
    "splits = grouped_data.randomSplit([0.6, 0.4], seed=413)\n",
    "\n",
    "# The 'splits' list now contains two DataFrames with the specified ratios\n",
    "train_groups = splits[0]\n",
    "test_groups = splits[1]\n",
    "#train_groups.show(10)\n",
    "\n",
    "# Join the train and test groups with the original data to get the final train and test sets\n",
    "train_data = data.join(train_groups, on=\"id\")\n",
    "test_data = data.join(test_groups, on=\"id\")\n",
    "\n",
    "# Show the resulting train and test DataFrames\n",
    "print(\"Train DataFrame:\")\n",
    "train_data.select(\"id\",\"gid\").show(2)\n",
    "print(\"Test DataFrame:\")\n",
    "test_data.select(\"id\",\"gid\").show(2)\n",
    "\n",
    "\n",
    "# ---- engineering x & y based on training data ----\n",
    "# --- y ---\n",
    "indexer = StringIndexer(inputCol=\"diagnosis\", outputCol=\"diagnosis_id\")\n",
    "indexerModel = indexer.fit(train_data)\n",
    "train_data = indexerModel.transform(train_data)\n",
    "test_data = indexerModel.transform(test_data)\n",
    "\n",
    "# --- x ---\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(train_data)\n",
    "train_data = scalerModel.transform(train_data)\n",
    "test_data = scalerModel.transform(test_data)\n",
    "\n",
    "\n",
    "# encoder = OneHotEncoder(inputCol=\"diagnosis_id\", outputCol=\"diagnosis_onehot\")\n",
    "# encoderModel = encoder.fit(data)\n",
    "# data = encoderModel.transform(data)\n",
    "# data.select(\"scaledFeatures\",\"diagnosis_onehot\").show(2, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [1.3128433255771863,0.15474126849459116]\n",
      "Intercept: -6.415834114503757\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "lr = LogisticRegression(labelCol='diagnosis_id',\n",
    "                        featuresCol='scaledFeatures',\n",
    "                        maxIter=1000, \n",
    "                        regParam=0.1, \n",
    "                        elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(train_data)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------+\n",
      "|probability                             |prediction|\n",
      "+----------------------------------------+----------+\n",
      "|[0.8148024469230046,0.18519755307699537]|0.0       |\n",
      "|[0.5185936559323403,0.4814063440676597] |0.0       |\n",
      "|[0.4370845371764407,0.5629154628235593] |1.0       |\n",
      "|[0.5681036046827329,0.4318963953172671] |0.0       |\n",
      "|[0.5701903017591848,0.42980969824081516]|0.0       |\n",
      "+----------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Area under ROC Curve: 0.8345588235294118\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# compute predictions. this will append column \"prediction\" to dataframe\n",
    "lrPred = lrModel.transform(test_data)\n",
    "lrPred.select('probability','prediction').show(5,truncate=False)\n",
    "\n",
    "# set up evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",\n",
    "                                          labelCol=\"diagnosis_id\",\n",
    "                                          metricName=\"areaUnderROC\")\n",
    "\n",
    "# pass to evaluator the DF with predictions, labels\n",
    "auc = evaluator.evaluate(lrPred)\n",
    "\n",
    "print(\"Area under ROC Curve:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part II: Regression (5 POINTS)\n",
    "\n",
    "In this project, you will work with the California Home Price dataset to train a regression model and predict median home prices. Here are the specifications and grading breakdown:\n",
    "\n",
    "- Scale the response variable median_house_value, dividing by 100000 (1 PT)\n",
    "\n",
    "- Split data into train set (80%), test set (20%) using seed=314 (1 PT)\n",
    "\n",
    "- Add new predictor: `rooms_per_household`\n",
    "\n",
    "- In the training set, select all of these features and standardize them: (1 PT)\n",
    "\n",
    "feats = [\"total_bedrooms\", \n",
    "         \"population\", \n",
    "         \"households\", \n",
    "         \"median_income\", \n",
    "         \"rooms_per_household\"]\n",
    "\n",
    "- Fit a linear regression model on the training set with these parameters:\n",
    "\n",
    "  - maxIter=10\n",
    "  - regParam=0.3\n",
    "  - elasticNetParam=0.8  \n",
    "\n",
    "\n",
    "- Compute the MSE on the test set (2 PTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILEPATH2 = 'cal_housing_data_preproc_w_header.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter code and solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+--------------------+--------------------+\n",
      "|median_house_value|     median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|rooms_per_household|            features|      scaledFeatures|\n",
      "+------------------+------------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+--------------------+--------------------+\n",
      "|           0.14999|             0.536|              36.0|       98.0|          28.0|      18.0|       8.0|   40.31|  -123.17|              12.25|[28.0,18.0,8.0,0....|[0.06602916111827...|\n",
      "|           0.14999|            1.6607|              16.0|      255.0|          73.0|      85.0|      38.0|   39.71|  -122.74| 6.7105263157894735|[73.0,85.0,38.0,1...|[0.17214745577263...|\n",
      "|           0.14999|               2.1|              19.0|      619.0|         239.0|     490.0|     164.0|    36.4|  -117.02|  3.774390243902439|[239.0,490.0,164....|[0.56360605383096...|\n",
      "|             0.175|            2.3667|              39.0|      493.0|         168.0|     259.0|     138.0|   34.15|  -118.33|  3.572463768115942|[168.0,259.0,138....|[0.39617496670963...|\n",
      "|             0.225|0.7916999999999998|              52.0|      107.0|          79.0|     167.0|      53.0|   37.95|  -121.29|  2.018867924528302|[79.0,167.0,53.0,...|[0.18629656172655...|\n",
      "|             0.225|            1.0918|              52.0|      845.0|         451.0|    1230.0|     375.0|   32.71|  -117.16| 2.2533333333333334|[451.0,1230.0,375...|[1.06354113086931...|\n",
      "|             0.225|            2.7138|               8.0|     9975.0|        1743.0|    6835.0|    1439.0|   35.43|  -116.57|  6.931897150799166|[1743.0,6835.0,14...|[4.11031527961245...|\n",
      "|              0.25|0.8570999999999998|              21.0|       44.0|          33.0|      64.0|      27.0|   32.79|  -114.65| 1.6296296296296295|[33.0,64.0,27.0,0...|[0.07782008274653...|\n",
      "|             0.266|            2.3013|              34.0|     1440.0|         309.0|     808.0|     294.0|   35.13|  -119.45| 4.8979591836734695|[309.0,808.0,294....|[0.72867895662664...|\n",
      "|             0.269|            2.1955|              46.0|     2745.0|         543.0|    1423.0|     482.0|   35.13|  -119.46|  5.695020746887967|[543.0,1423.0,482...|[1.28049408882935...|\n",
      "+------------------+------------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# load data\n",
    "data = spark.read.csv(DATA_FILEPATH2, header=True, inferSchema=True)\n",
    "# data.show(2)\n",
    "\n",
    "\n",
    "# Scale the response variable median_house_value, dividing by 100000 \n",
    "data = data.withColumn(\"median_house_value\",  col(\"median_house_value\")/100000)\n",
    "\n",
    "# Add new predictor: rooms_per_household\n",
    "data = data.withColumn(\"rooms_per_household\",  col(\"total_rooms\")/col(\"households\"))\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"total_bedrooms\", \"population\", \"households\", \"median_income\", \"rooms_per_household\"],\n",
    "                            outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "\n",
    "\n",
    "# Split data into train set (80%), test set (20%) using seed=314\n",
    "splits = data.randomSplit([0.8, 0.2], seed=413)\n",
    "train_data = splits[0]\n",
    "test_data = splits[1]\n",
    "\n",
    "# In the training set, select all of these features and standardize them:\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(train_data)\n",
    "train_data = scalerModel.transform(train_data)\n",
    "test_data = scalerModel.transform(test_data)\n",
    "train_data.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.0,0.0,0.0,0.27621981890278596,0.0]\n",
      "Intercept: 0.9994485205489783\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol='features',         # feature vector name\n",
    "                      labelCol='median_house_value',  # target variable name\n",
    "                      maxIter=10,\n",
    "                      regParam=0.3, \n",
    "                      elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(train_data)\n",
    "\n",
    "# Print the weights and intercept for linear regression\n",
    "print(\"Weights: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+--------------------+--------------------+------------------+\n",
      "|median_house_value|median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|rooms_per_household|            features|      scaledFeatures|        prediction|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+--------------------+--------------------+------------------+\n",
      "|           0.14999|       4.1932|              52.0|      803.0|         267.0|     628.0|     225.0|   34.24|  -117.86|  3.568888888888889|[267.0,628.0,225....|[0.62963521494924...|2.1576934651721404|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+--------------------+--------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "--------------------\n",
      "METRICS\n",
      "Mean Squared Error: 0.7557421874855437\n",
      "R Squared: 0.4271187844987485\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# compute predictions. this will append column \"prediction\" to dataframe\n",
    "lrPred = lrModel.transform(test_data)\n",
    "lrPred.show(1)\n",
    "\n",
    "ev = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"median_house_value\")\n",
    "\n",
    "print('-'*20)\n",
    "print(\"METRICS\")\n",
    "print(\"Mean Squared Error:\", ev.evaluate(lrPred, {ev.metricName: \"mse\"}))\n",
    "print(\"R Squared:\", ev.evaluate(lrPred, {ev.metricName:'r2'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
